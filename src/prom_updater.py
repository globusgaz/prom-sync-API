# src/prom_updater.py
import os
import asyncio
import aiohttp
import lxml.etree as ET
import orjson
from dotenv import load_dotenv

load_dotenv()

PROM_API_TOKEN = os.getenv("PROM_API_TOKEN")
PROM_EDIT_URL = "https://my.prom.ua/api/v1/products/edit"

HEADERS = {
    "Authorization": f"Bearer {PROM_API_TOKEN}",
    "Content-Type": "application/json",
}

BATCH_SIZE = 100
MAX_CONCURRENT = 5


# ==== –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ñ—ñ–¥—ñ–≤ ====
async def fetch_feed(session, url: str):
    try:
        async with session.get(url, headers={"User-Agent": "Mozilla/5.0"}) as resp:
            resp.raise_for_status()
            text = await resp.text()
            root = ET.fromstring(text.encode("utf-8"))
            offers = root.findall(".//offer")
            print(f"‚úÖ {url} ‚Äî {len(offers)} —Ç–æ–≤–∞—Ä—ñ–≤")
            return offers
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É {url}: {e}")
        return []


async def load_all_feeds(file_path="feeds.txt"):
    with open(file_path, "r") as f:
        urls = [line.strip() for line in f if line.strip()]
    print(f"üîó Found {len(urls)} feed URLs in {file_path}")

    async with aiohttp.ClientSession() as session:
        tasks = [fetch_feed(session, url) for url in urls]
        results = await asyncio.gather(*tasks)
        return [offer for sublist in results for offer in sublist]


# ==== –í—ñ–¥–ø—Ä–∞–≤–∫–∞ –±–∞—Ç—á—ñ–≤ ====
payload_logged = False  # –≥–ª–æ–±–∞–ª—å–Ω–∏–π –ø—Ä–∞–ø–æ—Ä–µ—Ü—å –¥–ª—è –ª–æ–≥—É–≤–∞–Ω–Ω—è –ª–∏—à–µ 1-–≥–æ payload


async def send_batch(session, batch, batch_num):
    global payload_logged
    try:
        payload = {"products": batch}

        if not payload_logged:  # –ø–æ–∫–∞–∑–∞—Ç–∏ —Ç—ñ–ª—å–∫–∏ –ø–µ—Ä—à–∏–π payload
            print("DEBUG: –ø—Ä–∏–∫–ª–∞–¥ payload –¥–æ Prom:")
            print(orjson.dumps(payload, option=orjson.OPT_INDENT_2).decode())
            payload_logged = True

        async with session.post(PROM_EDIT_URL, headers=HEADERS, data=orjson.dumps(payload)) as resp:
            text = await resp.text()
            if resp.status != 200:
                print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ Prom {resp.status}: {text}")
            else:
                try:
                    data = orjson.loads(text)
                    updated = len(data.get("processed_ids", []))
                except Exception:
                    updated = len(batch)
                print(f"‚úÖ Batch {batch_num} ‚Äî –æ–Ω–æ–≤–ª–µ–Ω–æ {updated} —Ç–æ–≤–∞—Ä—ñ–≤")
    except Exception as e:
        print(f"‚ö†Ô∏è –í–∏–Ω—è—Ç–æ–∫ –ø—Ä–∏ –≤—ñ–¥–ø—Ä–∞–≤—Ü—ñ batch {batch_num}: {e}")


async def update_products(updates):
    if not updates:
        print("üö´ –ù–µ–º–∞—î –æ–Ω–æ–≤–ª–µ–Ω—å")
        return

    batches = [updates[i:i + BATCH_SIZE] for i in range(0, len(updates), BATCH_SIZE)]

    connector = aiohttp.TCPConnector(limit=MAX_CONCURRENT)
    async with aiohttp.ClientSession(connector=connector) as session:
        tasks = [send_batch(session, batch, idx + 1) for idx, batch in enumerate(batches)]
        await asyncio.gather(*tasks)


# ==== –ì–æ–ª–æ–≤–Ω–∞ –ª–æ–≥—ñ–∫–∞ ====
async def main():
    offers = await load_all_feeds()
    print(f"üì¶ –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–≤–∞—Ä—ñ–≤: {len(offers)}")

    updates = []
    for offer in offers:
        external_id = offer.get("id")
        price = offer.findtext("price")
        quantity = offer.findtext("quantity")

        if external_id and price:
            updates.append(
                {
                    "external_id": external_id,
                    "price": float(price),
                    "quantity_in_stock": int(quantity) if quantity else 0,
                    "presence": "available" if quantity and int(quantity) > 0 else "not_available",
                }
            )

    print(f"üõ†Ô∏è –ì–æ—Ç–æ–≤–æ {len(updates)} –æ–Ω–æ–≤–ª–µ–Ω—å")
    await update_products(updates)


if __name__ == "__main__":
    asyncio.run(main())
